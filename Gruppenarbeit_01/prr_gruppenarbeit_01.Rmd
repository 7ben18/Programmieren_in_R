---
title: "Programmieren in R: Abgabe 1"
author: "Gabriel Torres Gamez, Si Ben Tran, Patrick Schürmann"
output:
  html_document:
    toc: true
    toc_float: true
---

# Lineare und logistische Regressionen in R

Für die erste Abgabe im Modul PRR beschäftigen wir uns mit (multiplen) linearen und logistischen Regressionen. Diese Funktionen wenden wir auf den Datensatz "Carseats" an. Datensatz von ISRL2.
Wir haben uns für diese Methoden entschieden, weil sie ein zentraler Bestandteil von Data Science sind. Sie wird verwendet, um Zusammenhängen in Datensätze zu verstehen und um Prognosen erstellen zu können. Diese Regressionen sind im R Grundpakte enthalten. Es muss also keine separate Library installiert und geladen werden.

Carseats ist ein generischer Datensatz, der im Buch "An Introduction to Statistical Learning" verwendet wird. Dieses Buch erarbeiten wir in diesem Studiengang im Modul "Statistisches Lernen". Dort wird er zwar hauptsächlich angewendet um Decision Trees zu erstellen. Dank 11 Variablen und 400 Observationen eignet er sich aber auch gut für Regressionen. Für eine genau Erklärung der Variablen verweisen wir auf die Homepage https://rdrr.io/cran/ISLR2/man/Carseats.html. Für die unten stehenden Hypothesen werden wir mit folgenden Variablen arbeiten:

- Sales: Verkaufte Anzahl Kindersitze pro Standort in 1'000 Stück
- CompPrice: Verkaufte Anzahl Kindersitze von Konkurrenten in 1'000 Stück
- Income: Das Lohnniveau der Einzugsgebiete der Standorte in 1'000 Dollar
- Advertising: Lokale Marketingausgaben in 1'000 Dollar
- Population: Bevölkerungsgrösse der Standorte in 1'000 Personen
- CompPrice: Die Preise für Kindersitze der lokalen Konkurrenten
- Price: Preis pro Kindersitz an den Standorten
- ShelveLoc: Ein Faktor der die Qualität des Produktes einteilt
- Age: Durchschnittsalter der lokalen Bevölkerung
- Education: Bildungslevel an jedem Standort
- Urban: Klassifikation, ob sich der Standort in einer städtischen oder ländlichen Region befindet
- US: Ein Indikator der angbit, ob sich das Geschäft in US befindet oder nicht

# Auflistung der Hypothesen
Zur Analyse des Datensatzes haben wir uns folgende Fragen gestellt:
1. Ist Sales abhaengig von Income/Advertising/Population der Region? (Lineare Regression)
2. Hat die Differenz von Comprice und Price einen Einfluss auf den Verkauf? (Lineare Regression)
3. Ist Sales durch mehrere Features abhaengig? (Multiple Lineare Regression)
4. Werden in urbanen Regionen mehr Kindersitze gekauft? (Logistische Regression)

# Vorbereitung
In diesem Kapitel installieren wir die notwendigen Libraries und verschaffen uns einen Überlick über den Carseats Datensatz.

## Installationen
```{r}
# Libraries installieren und laden
library(ISLR2)
library(tidyverse)
```
In der Library ISLR2 befindet sich der Datensatz. 
Tidyverse wird für die Darstellung des Datensatzes und Visualisierungen benötigt.

## Überblick Datensatz Carseats
In einem ersten Schritt ist es wichtig den Datensatz zu verstehen, bevor Visualisierungen und Modelle erstellt werden.

### Daten einlesen
```{r}
# Carseats Daten einlesen
data(Carseats)
```

### Kopfzeile Carseats ausgeben 
```{r}
# Die ersten 6 Oberservationen ausgeben
head(Carseats)
```

### Fusszeile des Datensatzes ausgeben
```{r}
# Die letzten 6 Observationen ausgeben
tail(Carseats)
```
Durch die Ausgabe von Kopfzeile und Fusszeile kommen wir mit dem Datensatz in Beruehrung und koennen ein Bild der Attribute sowie Observationen machen.

### Informationen zum Datensatz herausfinden
```{r}
# Informationen zum DataFrame ausgeben
str(Carseats)
```
Durch str wird und die grösse des DataFrames angezeigt sowie die anzahl an Features (Spalten). Weiter erkennen wir von jeder Spalte den Datenyp. Weiter erkennen wir auch einige Werte die in der Spalte vorkommen. 

### Summarische Zusammenfassung von Carseats
```{r}
# Statistische Kennzahlen 
summary(Carseats)
```
wir geben von jeder numerischen Spalte die summarischen Statistiken aus. 
Darin enthalten sind: Minimum, 1. Quantil, Median, Mittelwert, 3. Quantil, Maximum.
Im summary() werden auch die Anzahl fehlende Werte angezeigt. 
In unserem Fall besitzt der Datensatz keine fehlende Werte.
Die Aufbereitung der Daten entfällt deshalb.

### Visualisierung der Daten

#### Verteilung der Sales
```{r}
ggplot(data = Carseats, aes(x = Sales)) + 
  geom_density(fill = "lightgreen", color = "black", alpha = 0.3) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```
Hier in diesem Dichte-Diagramm erkennen wir die Verteilung der numerischen Variabel Sales.
Die y Achse ist die Wahrscheinlichkeit für das Auftreten einer numerischen Sales Variabel. 
Aus diesem Grund wird die Dichtefunktion auch oft Wahrscheinlichkeitsdichtefunktion genannt. 
Sie ist eine gegelättete Version des Histogramm und wird oft nach dem gleichen Konzept verwendet. 

#### Verteilung der Sales anhand der Variable Urban
```{r}
ggplot(data = Carseats, aes(x = Sales, fill = Urban)) + 
  geom_density(alpha = 0.3) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```
Analog zur ersten Visualisierung betrachten wir hier die Dichteverteilung von Sales mit der Auftrennung
von Urban (ja/nein), sprich können wir mit diesem Plot unterschiede zwischen Urbanen Regionen (städtische oder ländliche)
Sales betrachten. In unserem Fall gibt es keinen grosse Unterschied, man erkennt ganz leicht das bei
Sales in nicht ländlichen Regionen diese im Sales Bereich 8 grösser ist als bei Städtischen Regionen. 

#### Multiplots Verteilungen von Sales
```{r}
ggplot(Carseats, aes(x = Sales, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```
In diesem Multiplot unterteilen wir die Dichteverteilung nach weiteren Kategorien, 
wie Urban und ShelveLoc. Auch unterteilen wir farblich, ob sich das Geschaeft in den US 
befindet oder nicht, damit wir ein Gefuehl fuer die Sales Verteilung mit verschiedenen 
Kategorischen Werten kriegen. Aufaellig ist, dass die Verteilungen bei ShelveLoc Bad 
deutlich weiter Links sind (Mittelwert tiefer als bei ShelveLoc Medium und Good). 
Auch erkennen wir bei ShelveLoc Good, das die Kindersitze in den nicht US laendern 
weniger verkauft werden als in den US selber. 

#### Multiplots Verteilungen von Price
```{r}
ggplot(Carseats, aes(x = Price, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Price",
       y = "Anzahl",
       title = "Verteilung von Price unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```
Analog zum Sales multiplot erstellen wir ein Multiplot von Price, um eine Uebersicht von Price 
mit unterschiedlichen Kategorien zu kriegen. Wir erkennen, dass bei ShelveLoc Good und Urbanen 
Regionen die Preise in den US deutlich hoeher sind als in nicht US Laendern. Auch erkennen wir
bei ShelveLoc Bad und nicht Urbanen Regionen, das die Preise fuer einen Kindersitz bei nicht US
Laendern teurer sind.

# Hypothese 1: Einfach lineare Regression
Ist Sales abhaengig von Income/Advertising/Population der Region?

## Sales vs. Income
### Visualisierung von Sales und Income
```{r}
plot(Sales ~ Income, data = Carseats, 
     main = "Sales vs Income", pch=21, bg = "lightblue")
```
Bevor wir mit der Linearen Regression beginnen, mochten wir zuerst einen Streudiagramm 
von beiden Variabeln erstellen um die Zusammenhaenge zu visualiseren. Hier erkennen wir im 
Plot, dass die Variabeln Einkommen und Sales nicht besonders stark korrelieren, 
da die Punkte ueberall im Plot verstreut sind. 

### Modell erstellen
```{r}
lm.fit1 <- lm(Sales ~ Income, data = Carseats)
```
Mit dem Befehl "lm() wird das Modell erstellt und trainiert.

### Auswertung vom Modell
```{r}
summary(lm.fit1)
```
R hat nun die linear Regression berechnet. Die wichtigsten Werte lassen sich mit der Funktion summary() anzeigen.
Der Intercept beträgt 6.44 und die Steigung 0.015. Pro 1'000 Dollar höheres Einkommen steigt der 
Verkauf um 15 Stück. Dies erscheint im Verhältnis zum Intercept nach wenig, es müsste darüber
diskutiert werden, ob dieser Erkenntnis für einen möglichen Kunden aussagekräftig ist.
Anhand vom P Wert(0.00231) lässt sich aber feststellen, dass die Steigung siginfikant ist. 
Das Einkommen der Region hat also einen Einfluss auf die Verkaufszahlen. 
Vor einer endültigen Aussage müssen aber weitere Untersuchungen, wie die Überprüfung der Residuen, durchgeführt werden.
Diese Untersuchung führen wir später durch. Vorab stellen wir weitere Methoden der linearen Regression vor.

### Übersicht der Modell Methoden
```{r}
names(lm.fit1)
```
Das Modell enthält weitere Berechnungen und Kennzahlen. Names() zeigt eine Übersicht dieser Methoden.

### Koeffzienten vom Modell ausgeben
```{r}
lm.fit1$coefficients
```
Die oben ersichtlichen Koeffizienten lassen sich direkt via $coefficients abrufen.

### Konfidenzintervall für Koeffizienten angeben
```{r}
confint(lm.fit1)
```
Wertvoll ist die Berechnung der Konfidenzintervalle, die im Summary nicht angezeigt wird. Diese gibt an, in welchem Bereich sich die Koeffizienten mit einer Wahrscheinlichkeit von 95% befinden.

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
# Streudiagramm erstellen
plot(Sales ~ Income, data = Carseats, 
     main = "Sales vs Income", pch=21, bg = "lightblue")
# Lineares Modell hinzufügen 
abline(lm.fit1, col = "red")
```
Das Modell lässt sich direkt mit der Plotfunktion darstellen. Mit abline() lassen sich Linien zeichnen, in diesem Fall die lineare Regression. Im Plot wird die Steigung ersichtlich, man sieht aber auch die starke Streuung.

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)  
```
Nun, da wir unser Modell haben, können wir feststellen, ob es ein gutes Modell ist, 
indem wir bei den Residuen folgende 3 Bedingungen überprüfen:

Der Fehler sollte nicht abhängig sein
Der Erwartungswert sollte bei 0 liegen.
Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. 
Der Median der Residuen entspricht -0.18, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die 
Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. 
Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

Ueberpreufen vom Erwartungswert =  0 + Normalverteilt
```{r}
hist(lm.fit1$residuals, 
     main = "Verteilung der Residuen", col = "lightblue") 
```
### Interpretation
Wir erkennen aufgrund des Streudiagramm, dass eine Korrelation zwischen einkommen und Sales kaum vorhanden sind. Das Lineare Modell hat einen R^2 von 0.02, dies bestaetit die Aussage. 

## Sales vs. Population
### Visualisierung von Sales und Population
```{r}
plot(Sales ~ Population, data = Carseats, 
     main = "Sales vs Population", pch=21, bg = "lightblue")
```
Bevor das Modell erstellt wird, visualisieren wir die beiden Variabeln Population und Sales.

### Model erstellen und Auswertung
```{r}
lm.fit2 <- lm(Sales ~ Population, data = Carseats)
summary(lm.fit2)
```
Die Einfluss der Population ist nicht signifikant. Der folgende Plot bestätigt, 
dass die Steigung 0 beträgt. 

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ Population, data = Carseats, 
     main = "Sales vs Population", pch=21, bg = "lightblue")
abline(lm.fit2, col = "red")
```
Hier in diesem Plot stellen wir die Population mit Sales sowie dem Modell zusammen. Wir erkennen, dass die Punkte sich deutlich von der roten Linie (Model) streuen

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit2) 
```
Auch in diesem Model ueberpruefen wir die folgenden Kriterien des Models.

Der Fehler sollte nicht abhängig sein
Der Erwartungswert sollte bei 0 liegen.
Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. 
Der Median der Residuen entspricht -0.0597, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die 
Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. 
Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

```{r}
hist(lm.fit2$residuals, 
     main = "Verteilung der Residuen", col = "lightblue") 
```
### Interpretation
Leider auch hier, aehnlich wie zu Sales vs. Income erkennen wir keinen Trend. Der R^2 Score
betraegt hier 0.002547, also auch kein Gutes Model. 

## Sales vs. Advertising
### Visualisierung von Sales und Population
```{r}
plot(Sales ~ Advertising, data = Carseats, 
     main = "Sales vs Advertising", pch=21, bg = "lightblue")
```
Bevor das Modell erstellt wird, visualisieren wir die beiden Variabeln Advertising und Sales.

### Modell erstellen und Auswertung
```{r}
lm.fit3 <- lm(Sales ~ Advertising, data = Carseats)
summary(lm.fit3)
```

### Visualisierung der Daten mit dem Regressions Modell
```{r}
plot(Sales ~ Advertising, data = Carseats, 
     main = "Sales vs Advertising", pch=21, bg = "lightblue")
abline(lm.fit3, col = "red")
```
Leider erkennen wir schon hier, dass es keinen Trend von Sales vs. Advertising vorhanden ist.
Vollstaendigkeitshalber fuehren wir die Analysen trotzdem durch.

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit3) 
```
Auch in diesem Model ueberpruefen wir die folgenden Kriterien des Models.

Der Fehler sollte nicht abhängig sein
Der Erwartungswert sollte bei 0 liegen.
Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. 
Der Median der Residuen entspricht -0.1037, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die 
Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. 
Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

```{r}
hist(lm.fit3$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```
### Interpretation
Leider auch bei Advertising und Sales ist kein linearer Zusammenhang erslichtlich. In der 3. Hypothese 
beschaeftigen wir uns mit der multiplen Linearen Regression, diese wird bestimmt besser sein als bei der
Linearen Regression. 

## Interpretation von Hypothese 1

Keiner der Linearen Modelle hat einen Linearen Zusammenhang. Mit einem einfachen Linearen Modell
ist es nicht moeglich die Sales vorhherzusagen. 


# Hypothese 2: Einfach lineare Regression mit berechneten Daten
Hat die Differenz von Comprice und Price einen Einfluss auf den Verkauf? (Lineare Regression)

## Sales vs. Preisdifferenz zu Konkurrenz
### Berechnung der Spalte DiffPrice
Hinzufügen einer Spalte mit der Differenz zwischen unserem Preis und der der Konkurrenz.
```{r}
Carseats <- Carseats %>% mutate(DiffPrice = Price - CompPrice) 
str(Carseats)
```


### Visualisierung von Sales und DiffPrice
```{r}
plot(Sales ~ DiffPrice, data = Carseats, 
     main = "Sales vs DiffPrice", pch=21, bg = "lightblue")
```
In dieser Visualisierung von DiffPrice und Sales erkennen wir, das due Streupunkte einem gewissen Trend folgen. 

### Model erstellen und Auswertung
```{r}
lm.fit4 <- lm(Sales ~ DiffPrice, data = Carseats)
summary(lm.fit4)
```
Die Auswertung zeigt, das dieses Modell einen R^2 von 0.3575 hat. Dies ist nicht besonders gut, dennoch besser als die ersten drei Modelle.

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ DiffPrice, data = Carseats, 
     main = "Sales vs DiffPrice", pch=21, bg = "lightblue")
abline(lm.fit4, col = "red")
```
Hier in diesem Diagramm erkennen wir, dass das Modell einen negativen Steigung hat. Dies erkennen wir einerseits an der Steigung im Summary aber auch bei der Visualisierung. 

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit4) 
```
Auch in diesem Model ueberpruefen wir die folgenden Kriterien des Models.

Der Fehler sollte nicht abhängig sein
Der Erwartungswert sollte bei 0 liegen.
Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. 
Der Median der Residuen entspricht -0.2678, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die 
Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. 
Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

```{r}
hist(lm.fit4$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```
## Interpretation der Hypothese 2
Die zweite Hypothese hat einen deutlich besseren R^2 Score verglichen zu den ersten drei Modellen. 
Es ist interessant zu erkennen, dass der Preisunterschied zwischen der Konkurrenz und dem Produkt 
einen ganz leichten negativen Trend zeigt. Nichts destotrotz, sind wir mit diesem Modell nicht zufrieden und begeben uns direkt in die Hypothese 3. 

# Hypothese 3: Multiple lineare Regression

## Sales vs. mehrere Features

Ist Sales durch mehrere Features abhaengig?
Nachdem wir festgestellt haben, dass sich Sales nicht durch einzelne Variabeln erklären lasst, untersuchen wir die
Möglichkeit, dass sie eine Ergebnis aus mehreren Faktoren bildet. Wir werden sie zuerst gegenüber zwei Variabel untersuchen. Danach schauen wir uns den ganzen Datensatz an und behalten danach die aussagekräftigsten Variabeln.

### Modelle erstellen und Auswertungen
In diesem Kapitel erstellen wir mehrere Modelle und wählen eines aufgrund einer ersten Auswertung für die weitere Untersuchung aus.

#### Sales vs. Income und Population

```{r}
mlr.fit1 <- lm(Sales ~ Income + Population, data = Carseats)
summary(mlr.fit1)
```
Zuerst untersuchen wir Sales gegenüber Income und der Population. In der Auswertung wird ersichtlich, dass Population keinen Zusammenhang aufweist. R^2 spricht ebenfalls gegen einen Zusammenhang in dieser Auswertung.

Die F-Statistik sagt aus, dass mit hoher Wahrscheinlichkeit (p-Wert < 0.6%) mindestens eine Variabel einen Einfluss hat. In unserem Fall ist das Income, war wir schon aus vorhergehenden Modellen wissen.


#### Sales vs. Income und Population und deren Multiplikation (Interaktion)

```{r}
mlr.fit2 <- lm(Sales ~ Income * Population, data = Carseats)
summary(mlr.fit2)
```
Hier haben wir untersucht, ob die Variabeln Income und Population nicht nur additiv einen Einfluss haben, sondern ob sie auch gemeinsam Wirken (Interaktion/Synergie). Das ist in diesem Fall aber nicht gegeben. 


#### Sales vs. alle Variabeln

```{r}
mlr.fit3 <- lm(Sales ~ ., data = Carseats)
summary(mlr.fit3)
```
Mit dem Code "Sales ~ ." können wir ein Regressionsmodell Sales gegenüber allen Variabeln erstellen.

Dank der Auswertung sehen wir weiterhin, dass sieben Variabeln in diesem Modell zum Sales beitragen. Population, Education und Location (Urban und US) haben keinen Einfluss. Wir werden deshalb die aussagekräftigsten vertieft überprüfen.

Die Fehlermeldung bei DiffPrice kommt daher, dass haben wir sie als Differenz von CompPrice und Price berechnet haben. Das lineare Modell erkennt, dass DiffPrice in diesem Fall keine zusätzliche Information zum Modell beiträgt. Deshalb weist es ein NA aus. 

#### Sales vs. aussagekräftigste Variabeln und DiffPrice

```{r}
mlr.fit4 <- lm(Sales ~ Income + Advertising + ShelveLoc + Age + DiffPrice, data = Carseats)
summary(mlr.fit4)
```
Durch die Auswahl der aussagekräftigsten Variabeln konnten wir ein Modell erstellen, dass Sales zuverlässig erklärt. Die verwendeten Variabeln haben wir aus dem Modell, das alle Features verwendet, entnommen. Diese haben dort den tiefsten P-Wert. In diesem Modell haben wir nur (auch in der F-Statistik) sehr tiefe p-Werte und ein ziemlich hohes R^2 von 0.87. Wir überprüfen deshalb nun die Residuen.

### Residuenanalyse

```{r}
par(mfrow = c(2,2))
plot(mlr.fit4)
```
Im Q-Q Plot wird bestätigt, dass es sich um ein lineares Modell handelt. Der erste Plot und das folgende Histogram bestätigen ebenfalls, dass die Residuen normalverteilt und einen Mittelwert von 0 aufweisen.


```{r}
hist(mlr.fit4$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```
## Interpreation von Hypothese 3
Unser letztes Modell hat gezeigt, dass mit einer sorgfältigne Auswahl der Variabeln Sales gut und zuverlässig erklärt werden kann. In der Praxis könnte noch mehr Kombinationen (Variabeln, Interaktionen, exponentielle Zusammenhänge etc)
untersucht werden. In dieser Abgabe lassen wir das weg, weil es sich lediglich um Wiederholungen der gezeigten
Funktionen handelt.


# Hypothese 4: Logistische Regression
Werden in urbanen Regionen mehr Kindersitze gekauft?

## Sales vs. Urban

### Klassifizierung erstellen

Um logistische Regressionen durchzuführen, benötigen wir binäre Variabeln. Im ersten Schritt ersetzen wir Yes/No von Urban durch 1/0.
```{r}
Carseats <- Carseats %>% 
  mutate(Urban_binaer = recode(Urban, "No" = 0, "Yes" = 1))
```

### Visualisierung von Sales und Urban
```{r}
# Plot Predicted data and original data points
ggplot(Carseats, aes(x=Sales, y=Urban)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "Urban",
       title = "Sales vs Urban",
       subtitle = "Carseats Datensatz")
```
Danke der visuellen Analyse von Sales und Urban wird ersichtlich, dass Urban:Yes in beide Richtungen stärker streut als Urban:No. Entsprechend wird das Modell kaum verlässliche Werte ergeben.

### Model erstellen und Auswertung
```{r}
logreg.fit1 <- glm(Urban_binaer ~ Sales, data = Carseats , family = binomial)
summary(logreg.fit1)
```
Wie wir aufgrund der Visualisierung erkannt haben, ergibt ein logistishes Regressionsmodell einen
grossen P>|z| Wert. Somit gibt es keinen logistischen Zusammenhang zwischen Sales und Urban.

Gegenüber der lineare Regression haben wir den Parameter "family = binomial" erfasst.

## Sales vs. ShelveLoc

Um ein Beispiel für eine logistische Regression zeigen zu können, untersuchen wir auch Sales und ShelveLoc

### Visualisierung von Sales und ShelveLoc
Zuerst für alles ShelveLoc, danach haben die Location "Medium" entfernt.
```{r}
Carseats %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) +
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "ShelveLoc",
       title = "Sales vs ShelveLoc",
       subtitle = "Carseats Datensatz")

# Remove Medium in ShelveLoc
Carseats_ex_medium <- Carseats %>% filter(ShelveLoc != "Medium")

Carseats_ex_medium %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "ShelveLoc",
       title = "Sales vs ShelveLoc",
       subtitle = "Carseats Datensatz")
```
Im ersten Plot wurde drei Variabeln dargestellt. Für ein gutes Vorzeigebeispiel werden wir nur Good und Bad
berücksichtigen.


### Klassifizierung erstellen
```{r}
Carseats_ex_medium <- Carseats_ex_medium %>% 
  mutate(ShelveLoc_binaer = recode(ShelveLoc, "Bad" = 0, "Good" = 1))
head(Carseats_ex_medium)
```
Wir haben "Medium" weggelassen und wieder eine binäre Klassifizierung erstellt.


### Model erstellen und Auswertung
```{r}
logreg.fit2 <- glm(ShelveLoc_binaer ~ Sales, data = Carseats_ex_medium , family = binomial)
summary(logreg.fit2)
```
Der sehr tiefe P-Wert weist auf eine hohe Zuverlässigkeit vom Modell hin.

Im folgenden Plot wird ersichtlich, dass das Modell bei allen Läden mit Sales > 7.96 den ShelveLoc als Good/1
klassifiziert. Die Berechnung des Wertes haben wir im folgenden Code Chunk auskommentiert.

### Visualisierung der Daten mit dem Regressions Modell
```{r}
ggplot(Carseats_ex_medium, aes(x=Sales, y=ShelveLoc_binaer)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "ShelveLoc",
       title = "Sales vs ShelveLoc",
       subtitle = "Carseats Datensatz") +
  stat_smooth(method="glm", formula = y ~ x, color="red", 
              se = FALSE, method.args = list(family=binomial)) + 
  geom_vline(xintercept = 7.9633)

# Berechnung der Decision Boundary
#-logreg.fit2$coefficients[1] / logreg.fit2$coefficients[2]

```

### Evaluierung vom logistischen Model mittels Confusions Matrix
Um die Genauigkeit des Modelles zu berechnen, erstellen wir eine Confusion Matrix.

#### Confusion Matrix erstellen
```{r}
# zuerst erstellen wir eine Prediction, mit den Testdaten

# Predicte ShelveLoc_binaer Werte 
logreg.fit2.probs <- predict(logreg.fit2, type = "response")

# Erstellt einen Vektor mit 181  Null Werten 
logreg.fit2.pred <- rep(0, 181)

# Uebreschreibt alle Werte die groesser sind als 0.5 als 1
logreg.fit2.pred[logreg.fit2.probs > .5] = 1

# Erstellen eine Kreuztabelle (Confusions Matrix) 

table(logreg.fit2.pred, Carseats_ex_medium$ShelveLoc_binaer)

```

Die Spalten entsprechen den tatsächlichen Werten und in den Zeilen befinden sich die vorhergesagten Werte.
Lesebeispiel: Das Modell hat 19 Werte als 0/Bad klassifiziert, die aber tatsächlich ein 1/Good gewesen sind.



#### Confusion Matrix Performenz Kennzahlen
##### Accuracy

Die Accuracy entspricht dem Anteil korrekter Prognosen an allen Prognosen.
```{r}
# Accuracy Berechnen
acc <- (82 + 66) / (82 + 19 + 14 + 66)
acc

```

##### Precision
Precision = True Positiv / (True Positiv + False Positiv)
Preicision ist, Anteil richtig positiv an allen Positiv

Precision entspricht der Anteil korrekter positiven/1 Prognosen an allen positiven Prognosen.
```{r}
# Precision Berechnen 
precision <- 66 / (66 + 14)
precision
```
##### Recall
Recall = True Positiv / (True Positiv + False Negativ)
Recall ist, Anteil richtig positiv an allen richtigen Vorhersagen

Recall entspricht der Anteil korrekter positiven/1 Prognosen an allen tatsächlich positiven Werten
```{r}
# Recall berechnen
recall <- 66 / (66 + 19)
recall
```


# Zusammenfassung

Mit unseren verwendeten Modellen konnten wir aufzeigen, dass Sales nicht von einzelnen Variabeln, aber von mehreren
zusammenhängt. Dank der multiplen linearen Regression konnten wir zuverlässigen die Einflussfaktoren aufzeigen.

Dank der logistischen Regression konnten wir darstellen, dass Urban keinen Einfluss hat. Mit dem Weglassen von Medium konnten wir für ShelveLoc ein zuverlässiges Modell erstellen.

