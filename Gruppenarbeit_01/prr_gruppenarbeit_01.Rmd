---
title: "Programmieren in R: Abgabe 1"
author: "Gabriel Torres Gamez, Si Ben Tran, Patrick Schürmann"
output:
  html_document:
    toc: true
    toc_float: true
---

# Lineare und logistische Regressionen in R

Für die erste Abgabe im Modul PRR beschäftigen wir uns mit (multiplen) linearen und logistischen Regressionen. Diese Funktionen wenden wir auf den Datensatz "Carseats" an. Datensatz von ISRL2.

Wir haben uns für diese Methoden entschieden, weil sie ein zentraler Bestandteil von Data Science sind. Sie wird verwendet, um Zusammenhängen in Datensätze zu verstehen und um Prognosen erstellen zu können. Diese Regressionen sind im R Grundpakte enthalten. Es muss also keine separate Library installiert und geladen werden.

Carseats ist ein generischer Datensatz, der im Buch "An Introduction to Statistical Learning" verwendet wird. Dieses Buch erarbeiten wir in diesem Studiengang im Modul "Statistisches Lernen". Dort wird er zwar hauptsächlich angewendet um Decision Trees zu erstellen. Dank 11 Variablen und 400 Observationen eignet er sich aber auch gut für Regressionen. Für eine genau Erklärung der Variablen verweisen wir auf die Homepage https://rdrr.io/cran/ISLR2/man/Carseats.html. Für die unten stehenden Hypothesen werden wir mit folgenden Variablen arbeiten:

- Sales: Verkaufte Anzahl Kindersitze pro Standort in 1'000 Stück
- Income: Das Lohnniveau der Einzugsgebiete der Standorte in 1'000 Dollar
- Advertising: Lokale Marketingausgaben in 1'000 Dollar
- Population: Bevölkerungsgrösse der Standorte in 1'000 Personen
- CompPrice: Die Preise für Kindersitze der lokalen Konkurrenten
- Price: Preis pro Kindersitz an den Standorten
- Urban: Klassifikation, ob sich der Standort in einer städtischen oder ländlichen Region befindet


## Hypothesen

Zur Analyse des Datensatzes haben wir uns folgende Fragen gestellt:

1) Ist Sales abhaengig von Income/Advertising/Population der Region? (Lineare Regression)
2) Hat die Differenz von Comprice und Price einen Einfluss auf den Verkauf? (Lineare Regression)
3) Ist Sales durch mehrere Features abhaengig? (Multiple Lineare Regression)
4) Werden in urbanen Regionen mehr Kindersitze gekauft? (Logistische Regression)


## Installationen

```{r}
# Libraries installieren und laden
library(ISLR2)
library(tidyverse)
```
In der Library ISLR2 befindet sich der Datensatz. 
Tidyverse wird für die Darstellung des Datensatzes und Visualisierungen benötigt.

## Datensatz Carseats verstehen
In einem ersten Schritt ist es wichtig den Datensatz zu verstehen, bevor Visualisierungen und Modelle erstellt werden.

### Daten einlesen
```{r}
data(Carseats)
```


### Kopfzeile Carseats ausgeben 
```{r}
# Die ersten 6 Oberservationen ausgeben
head(Carseats)
```

### Endes des Datensatzes ausgeben
```{r}
# Die letzten 6 Observationen ausgeben
tail(Carseats)
```
Durch die Ausgabe von Kopfzeile und Fusszeile kommen wir mit dem Datensatz in Beruehrung und koennen ein Bild der Attribute sowie Observationen machen.

### Informationen zum Datensatz herausfinden
```{r}
# Informationen zum DataFrame ausgeben
str(Carseats)
```
Durch str wird und die groesse des DataFrames angezeigt sowie die anzahl an Features (Spalten). Weiter erkennen wir von jeder Spalte den Datenype. 

### Zusammenfassung von Carseats
```{r}
# Statistische Kennzahlen 
summary(Carseats)
```
wir geben von jeder numerischen Spalte die summarischen Statistiken aus. Im summary() werden auch die Anzahl fehlende Werte angezeigt. In unserem Fall besitzt der Datensatz keine fehlende Werte.Die Aufbereitung der Daten entfällt deshalb.

### Verteilung der Sales
Dieses eignet sich, um ein Verständnis der Verteilung der Verkäufe zu erhalten.

#### Verteilung der Sales mittels Histogramm
```{r}
ggplot(data = Carseats, aes(x = Sales)) + 
  geom_histogram(fill = "lightgreen", color = "black", binwidth = 0.5) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```

#### Verteilung der Sales mittels Density Plot
```{r}
ggplot(data = Carseats, aes(x = Sales, fill = Urban)) + 
  geom_density(alpha = 0.5) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```

#### Multiplots Verteilungen
```{r}
ggplot(Carseats, aes(x = Sales, fill = US)) + 
  geom_density(alpha = 0.5) +
  facet_grid(Urban  ~ ShelveLoc)
```

## Hypothese 1

### Visualisierung

Bevor das Modell erstellt wird, visualisieren wir die beiden Variablen.

```{r}
plot(Sales ~ Income, data = Carseats)
```

### Modell erstellen

Mit dem Befehl "lm() wird das Modell erstellt und trainiert.

```{r}
lm.fit1 <- lm(Sales ~ Income, data = Carseats)
```

### Auswertung vom Modell

R hat nun die linear Regression berechnet. Die wichtigsten Werte lassen sich mit der Funktion summary() anzeigen.

```{r}
summary(lm.fit1)
```
Der Intercept beträgt 6.44 und die Steigung 0.015. Pro 1'000 Dollar höheres Einkommen steigt der Verkauf um 15 Stück. Dies erscheint im Verhältnis zum Intercept nach wenig, es müsste darüber diskutiert werden, ob dieser Erkenntnis für einen möglichen Kunden aussagekräftig ist. Anhand vom P Wert(0.00231) lässt sich aber feststellen, dass die Steigung siginfikant ist. Das Einkommen der Region hat also einen Einfluss auf die Verkaufszahlen. Vor einer endültigen Aussage müssen aber weitere Untersuchungen, wie die Überprüfung der Residuen, durchgeführt werden.

Diese Untersuchung führen wir später durch. Vorab stellen wir weitere Methoden der linearen Regression vor.

### Modell Methoden

Das Modell enthält weitere Berechnungen und Kennzahlen. Names() zeigt eine Übersicht dieser.

```{r}
names(lm.fit1)
```

### Koeffzienten vom Modell ausgeben

Die oben ersichtlichen Koeffizienten lassen sich direkt via $coefficients abrufen.

```{r}
lm.fit1$coefficients
```
### Konfidenzintervall fuer Koeffizienten angeben

Wertvoll ist die Berechnung der Konfidenzintervalle, die im Summary nicht angezeigt wird. Diese gibt an, in welchem Bereich sich die Koeffizienten mit einer Wahrscheinlichkeit von 5% befinden.

```{r}
confint(lm.fit1)
```


## Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)

Das Modell lässt sich direkt mit der Plotfunktion darstellen. Mit abline() lassen sich Linien zeichnen, in diesem Fall die lineare Regression. Im Plot wird die Steigung ersichtlich, man sieht aber auch die starke Streuung.

```{r}
plot(Sales ~ Income, data = Carseats)
abline(lm.fit1, col = "red")
```

### Residuenanalyse (Visualisierung ausbessern)
Nun, da wir unser Modell haben, können wir feststellen, ob es ein gutes Modell ist, indem wir bei den Residuen folgende 3 Bedingungen überprüfen:

Der Fehler sollte nicht abhängig sein
Der Erwartungswert sollte bei 0 liegen.
Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. Der Median der Residuen entspricht -0.18, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. Auch dieses Kriterium ist somit erfüllt.

```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)  
```

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen. Im folgenden Plot ist ersichtlich, dass die Verteilung einigermassen normalverteilt ist.

```{r}
hist(lm.fit1$residuals) # Ueberpreufen vom Erwartungswert =  0 + Normalverteilt
```

Wir werden nun das Modell für die Variablen Population und Advertising durchführen. Danach folgt eine Interpretation der Resultate und Erkenntnisse.


### Sales vs. Population

Da für die weiteren beiden Modelle die gleichen Untersuchungen durchgeführt werden, kommentieren wir diese nicht mehr ausführlich.

```{r}
lm.fit2 <- lm(Sales ~ Population, data = Carseats)
summary(lm.fit2)
```

Die Einfluss der Population ist nicht signifikant. Der folgende Plot bestätigt, dass die Steigung 0 beträgt. Wir werden diese Variable deshalb nicht weiter untersuchen.

```{r}
plot(Sales ~ Population, data = Carseats)
abline(lm.fit2, col = "red")
```

## Sales vs. Advertising
```{r}
lm.fit3 <- lm(Sales ~ Advertising, data = Carseats)
summary(lm.fit3)
```


```{r}
plot(Sales ~ Advertising, data = Carseats)
abline(lm.fit3, col = "red")
```


```{r}
plot(lm.fit3, which=1)
```

```{r}
hist(lm.fit3$residuals) 
# Ueberpreufen vom Erwartungswert =  0 + Normalverteilt
```
### Interpretation

Interperation: Sales ist nicht abhaengig von Advertising. Erkennbar im Tukeyanscombe Plot, Histogramm ist irreführend.


## Hypothese 2
Hat die Differenz von Comprice und Price einen Einfluss auf den Verkauf? (Lineare Regression)

Hinzufügen einer Spalte mit der Differenz zwischen unserem Preis und der der Konkurrenz.
```{r}
Carseats <- Carseats %>% mutate(DiffPrice = Price - CompPrice) 
str(Carseats)
```

### Streudiagramm erstellen von Diffprice vs. Sales
```{r}
plot(Sales ~ DiffPrice, data = Carseats)
```
### Linear Regressionsmodell erstellen
```{r}
lm.fit4 <- lm(Sales ~ DiffPrice, data = Carseats)
```

### Summarische Auswertung von lm.fit4
```{r}
summary(lm.fit4)
```

### Streudiagramm mit Linearem Modell
```{r}
plot(Sales ~ DiffPrice, data = Carseats)
abline(lm.fit4, col = "red")
```

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit4) 
```

### Histogramm der Residuen
```{r}
hist(lm.fit4$residuals) 
```

# Multiple lineare Regressionsmodell

## Hypothese 3

Ist Sales durch mehrere Features abhaengig?

```{r}
mlr.fit1 <- lm(Sales ~ Income + Population, data = Carseats)
summary(mlr.fit1)
```

```{r}
mlr.fit2 <- lm(Sales ~ Income * Population, data = Carseats)
summary(mlr.fit2)
```

```{r}
mlr.fit3 <- lm(Sales ~ ., data = Carseats)
summary(mlr.fit3)
```
Kommentar: Population hat definitv keinen Einfluss auf Sales, wir überprüfen deshalb andere Variablen.
DiffPrice haben wir als Differenz von CompPrice und Price berechnet. Das lineare Modell erkennt, dass DiffPrice in diesem Fall keine zusätzliche Information zum Modell beiträgt. Deshalb weist es ein NA aus. 

```{r}
mlr.fit4 <- lm(Sales ~ Income + Advertising + ShelveLoc + Age + DiffPrice, data = Carseats)
summary(mlr.fit4)
```
Durch die Auswahl der aussagekräftigsten Variabeln konnten wir ein Modell erstellen, dass Sales zuverlässig erklärt. Die verwendeten Variabeln haben wir aus dem Modell, das alle Features verwendet, entnommen. Diese haben dort den tiefsten P-Wert.

### Analyse der Residuen

```{r}
par(mfrow = c(2,2))
plot(mlr.fit4)
```

```{r}
hist(mlr.fit4$residuals, main = "Histogramm der Residuen", xlab = "Residuen")
```


#Logistische Regression

## Hypothese 4
Werden in urbanen Regionen mehr Kindersitze gekauft?

### Klassifizierung

```{r}
Carseats <- Carseats %>% 
  mutate(Urban_binaer = recode(Urban, "No" = 0, "Yes" = 1))
```

### Visualisierung mittels Streudiagramm
```{r}
# Plot Predicted data and original data points
ggplot(Carseats, aes(x=Sales, y=Urban)) + 
  geom_point() +
  labs(title = "Streudiagramm")
```
Eine Logistische Regression von Urban (Yes, No) ist mittels Sales nicht sinnvoll, da die Streupunkte
uebereinander liegen.

### Logistisches Modell erstellen
```{r}
logreg.fit1 <- glm(Urban_binaer ~ Sales, data = Carseats , family = binomial)
summary(logreg.fit1)
```
Wie Aufgrund der Visualisierung gesehen, ergibt ein logistishes Regressionsmodell einen
grossen P>|z| Wert. Somit gibt es keinen logistischen Zusammenhang zwischen Sales und Urban. 

### Scatterplot von Urban vs Population
```{r}
Carseats %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) +
  geom_point()

# Remove Medium in ShelveLoc
Carseats_ex_medium <- Carseats %>% filter(ShelveLoc != "Medium")

Carseats_ex_medium %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) + 
  geom_point()
```

### Klassifizierung
```{r}
Carseats_ex_medium <- Carseats_ex_medium %>% 
  mutate(ShelveLoc_binaer = recode(ShelveLoc, "Bad" = 0, "Good" = 1))
head(Carseats_ex_medium)
```

### Logistisches Modell fuer ShelveLoc erstellen
```{r}
logreg.fit2 <- glm(ShelveLoc_binaer ~ Sales, data = Carseats_ex_medium , family = binomial)
summary(logreg.fit2)
```

### Logistische Regression im Streudiagramm visualisieren
```{r}
ggplot(Carseats_ex_medium, aes(x=Sales, y=ShelveLoc_binaer)) + 
  geom_point() +
  stat_smooth(method="glm", formula = y ~ x, color="green", 
              se = FALSE, method.args = list(family=binomial)) + 
  geom_vline(xintercept = 7.9633)


# Berechnung der Decision Boundary
-logreg.fit2$coefficients[1] / logreg.fit2$coefficients[2]
```
Alle Sales die groesser als 7.9633 sind, werden bei der Logistischen Regression als Gut definiert. 

### Evaluierung vom logistischen Model mittels confusions Matrix
Wir erstellen ein neues logistisches Model und verwenden dafuer einen Trainingsdatensatz


### Confusion Matrix erstellen
```{r}
# zuerst erstellen wir eine preidiction, mit den Testdaten

# Predicte ShelveLoc_binaer Werte 
logreg.fit2.probs <- predict(logreg.fit2, type = "response")

# Erstellt einen Vektor mit 181  Null Werten 
logreg.fit2.pred <- rep(0, 181)

# Uebreschreibt alle Werte die groesser sind als 0.5 als 1
logreg.fit2.pred[logreg.fit2.probs > .5] = 1

# Erstellen eine Kreuztabelle (Confusions Matrix) 

table(logreg.fit2.pred, Carseats_ex_medium$ShelveLoc_binaer)

```
Spalten = Carseats_ex_medium$Shelvelo_binear => Tatsaechlichen Werte
Zeilen => Vorhergesagte Werte


### Confusion Matrix Performenz Kennzahlen
#### Accuracy
```{r}
# Accuracy Berechnen
acc <- (82 + 66) / (82 + 19 + 14 + 66)
acc

```

#### Precision
Precision = True Positiv / (True Positiv + False Positiv)
Preicision ist, Anteil richtig positiv an allen Positiv
```{r}
# Precision Berechnen 
precision <- 66 / (66 + 14)
precision
```
#### Reall
Recall = True Positiv / (True Positiv + False Negativ)
Recall ist, Anteil richtig positiv an allen richtigen Vorhersagen
```{r}
# Recall berechnen
recall <- 66 / (66 + 19)
recall
```