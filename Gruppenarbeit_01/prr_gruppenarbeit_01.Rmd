---
title: "Programmieren in R: Abgabe 1"
author: "Gabriel Torres Gamez, Si Ben Tran, Patrick Schürmann"
output:
  html_document:
    toc: true
    toc_float: true
---

# Lineare und logistische Regressionen in R

Für die erste Abgabe im Modul PRR beschäftigen wir uns mit (multiplen) linearen und logistischen Regressionen. Diese Funktionen wenden wir auf den Datensatz "Carseats" an. Datensatz von ISRL2.

Wir haben uns für diese Methoden entschieden, weil sie ein zentraler Bestandteil von Data Science sind. Sie wird verwendet, um Zusammenhängen in Datensätze zu verstehen und um Prognosen erstellen zu können. Diese Regressionen sind im R Grundpakte enthalten. Es muss also keine separate Library installiert und geladen werden.

Carseats ist ein generischer Datensatz, der im Buch "An Introduction to Statistical Learning" verwendet wird. Dieses Buch erarbeiten wir in diesem Studiengang im Modul "Statistisches Lernen". Dort wird er zwar hauptsächlich angewendet um Decision Trees zu erstellen. Dank 11 Variablen und 400 Observationen eignet er sich aber auch gut für Regressionen. Für eine genau Erklärung der Variablen verweisen wir auf die Homepage https://rdrr.io/cran/ISLR2/man/Carseats.html. Für die unten stehenden Hypothesen werden wir mit folgenden Variablen arbeiten:

- Sales: Verkaufte Anzahl Kindersitze pro Standort in 1'000 Stück
- Income: Das Lohnniveau der Einzugsgebiete der Standorte in 1'000 Dollar
- Advertising: Lokale Marketingausgaben in 1'000 Dollar
- Population: Bevölkerungsgrösse der Standorte in 1'000 Personen
- CompPrice: Die Preise für Kindersitze der lokalen Konkurrenten
- Price: Preis pro Kindersitz an den Standorten
- Urban: Klassifikation, ob sich der Standort in einer städtischen oder ländlichen Region befindet


# Auflistung der Hypothesen

Zur Analyse des Datensatzes haben wir uns folgende Fragen gestellt:

1) Ist Sales abhaengig von Income/Advertising/Population der Region? (Lineare Regression)
2) Hat die Differenz von Comprice und Price einen Einfluss auf den Verkauf? (Lineare Regression)
3) Ist Sales durch mehrere Features abhaengig? (Multiple Lineare Regression)
4) Werden in urbanen Regionen mehr Kindersitze gekauft? (Logistische Regression)

# Vorbereitung
In diesem Kapitel installieren wir die notwendigen Libraries und verschaffen uns einen Überlick über den Carseats Datensatz.

## Installationen

```{r}
# Libraries installieren und laden
library(ISLR2)
library(tidyverse)
```
In der Library ISLR2 befindet sich der Datensatz. 
Tidyverse wird für die Darstellung des Datensatzes und Visualisierungen benötigt.

## Überblick Datensatz Carseats
In einem ersten Schritt ist es wichtig den Datensatz zu verstehen, bevor Visualisierungen und Modelle erstellt werden.

### Daten einlesen
```{r}
data(Carseats)
```


### Kopfzeile Carseats ausgeben 
```{r}
# Die ersten 6 Oberservationen ausgeben
head(Carseats)
```

### Fusszeile des Datensatzes ausgeben
```{r}
# Die letzten 6 Observationen ausgeben
tail(Carseats)
```
Durch die Ausgabe von Kopfzeile und Fusszeile kommen wir mit dem Datensatz in Beruehrung und koennen ein Bild der Attribute sowie Observationen machen.

### Informationen zum Datensatz herausfinden
```{r}
# Informationen zum DataFrame ausgeben
str(Carseats)
```
Durch str wird und die grösse des DataFrames angezeigt sowie die anzahl an Features (Spalten). Weiter erkennen wir von jeder Spalte den Datenyp. Weiter erkennen wir auch einige Werte die in der Spalte vorkommen. 

### Summarische Zusammenfassung von Carseats
```{r}
# Statistische Kennzahlen 
summary(Carseats)
```
wir geben von jeder numerischen Spalte die summarischen Statistiken aus. 
Darin enthalten sind: Minimum, 1. Quantil, Median, Mittelwert, 3. Quantil, Maximum.
Im summary() werden auch die Anzahl fehlende Werte angezeigt. 
In unserem Fall besitzt der Datensatz keine fehlende Werte.
Die Aufbereitung der Daten entfällt deshalb.

### Visualisierung der Daten

#### Verteilung der Sales
```{r}
ggplot(data = Carseats, aes(x = Sales)) + 
  geom_density(fill = "lightgreen", color = "black", alpha = 0.3) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```

#### Verteilung der Sales anhand der Variable Urban
```{r}
ggplot(data = Carseats, aes(x = Sales, fill = Urban)) + 
  geom_density(alpha = 0.3) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```

#### Multiplots Verteilungen von Sales
```{r}
ggplot(Carseats, aes(x = Sales, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

#### Multiplots Verteilungen von CompPrice
```{r}
ggplot(Carseats, aes(x = CompPrice, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "CompPrice",
       y = "Anzahl",
       title = "Verteilung von CompPrice unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

#### Multiplots Verteilungen von Income
```{r}
ggplot(Carseats, aes(x = Income, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Income",
       y = "Anzahl",
       title = "Verteilung von Income unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

#### Multiplots Verteilungen von Population
```{r}
ggplot(Carseats, aes(x = Population, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Population",
       y = "Anzahl",
       title = "Verteilung von Population unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

#### Multiplots Verteilungen von Price
```{r}
ggplot(Carseats, aes(x = Price, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Price",
       y = "Anzahl",
       title = "Verteilung von Price unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

#### Multiplots Verteilungen von Age
```{r}
ggplot(Carseats, aes(x = Age, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Age",
       y = "Anzahl",
       title = "Verteilung von Age unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

#### Multiplots Verteilungen von Education
```{r}
ggplot(Carseats, aes(x = Education, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Education",
       y = "Anzahl",
       title = "Verteilung von Education unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```

# Hypothese 1: Einfach lineare Regression
Ist Sales abhaengig von Income/Advertising/Population der Region?

## Sales vs. Income
### Visualisierung von Sales und Income
Bevor das Model erstellt wird, visualisieren wir die beiden Variablen.

```{r}
plot(Sales ~ Income, data = Carseats, 
     main = "Sales vs Income", pch=21, bg = "lightblue")
```

### Modell erstellen

Mit dem Befehl "lm() wird das Modell erstellt und trainiert.

```{r}
lm.fit1 <- lm(Sales ~ Income, data = Carseats)
```

### Auswertung vom Modell

R hat nun die linear Regression berechnet. Die wichtigsten Werte lassen sich mit der Funktion summary() anzeigen.

```{r}
summary(lm.fit1)
```
Der Intercept beträgt 6.44 und die Steigung 0.015. Pro 1'000 Dollar höheres Einkommen steigt der Verkauf um 15 Stück. Dies erscheint im Verhältnis zum Intercept nach wenig, es müsste darüber diskutiert werden, ob dieser Erkenntnis für einen möglichen Kunden aussagekräftig ist. Anhand vom P Wert(0.00231) lässt sich aber feststellen, dass die Steigung siginfikant ist. Das Einkommen der Region hat also einen Einfluss auf die Verkaufszahlen. Vor einer endültigen Aussage müssen aber weitere Untersuchungen, wie die Überprüfung der Residuen, durchgeführt werden.

Diese Untersuchung führen wir später durch. Vorab stellen wir weitere Methoden der linearen Regression vor.

### Übersicht der Modell Methoden

Das Modell enthält weitere Berechnungen und Kennzahlen. Names() zeigt eine Übersicht dieser.

```{r}
names(lm.fit1)
```

### Koeffzienten vom Modell ausgeben

Die oben ersichtlichen Koeffizienten lassen sich direkt via $coefficients abrufen.

```{r}
lm.fit1$coefficients
```
### Konfidenzintervall für Koeffizienten angeben

Wertvoll ist die Berechnung der Konfidenzintervalle, die im Summary nicht angezeigt wird. Diese gibt an, in welchem Bereich sich die Koeffizienten mit einer Wahrscheinlichkeit von 5% befinden.

```{r}
confint(lm.fit1)
```


### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)

Das Modell lässt sich direkt mit der Plotfunktion darstellen. Mit abline() lassen sich Linien zeichnen, in diesem Fall die lineare Regression. Im Plot wird die Steigung ersichtlich, man sieht aber auch die starke Streuung.

```{r}
# Streudiagramm erstellen
plot(Sales ~ Income, data = Carseats, 
     main = "Sales vs Income", pch=21, bg = "lightblue")
# Lineares Modell hinzufügen 
abline(lm.fit1, col = "red")
```

### Residuenanalyse
Nun, da wir unser Modell haben, können wir feststellen, ob es ein gutes Modell ist, indem wir bei den Residuen folgende 3 Bedingungen überprüfen:

Der Fehler sollte nicht abhängig sein
Der Erwartungswert sollte bei 0 liegen.
Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. Der Median der Residuen entspricht -0.18, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. Auch dieses Kriterium ist somit erfüllt.

```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)  
```

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen. Im folgenden Plot ist ersichtlich, dass die Verteilung einigermassen normalverteilt ist.


Ueberpreufen vom Erwartungswert =  0 + Normalverteilt
```{r}
hist(lm.fit1$residuals, 
     main = "Verteilung der Residuen", col = "lightblue") 
```

### Interpretation
viel blabla blaaaaa

## Sales vs. Population

### Visualisierung von Sales und Population

Bevor das Modell erstellt wird, visualisieren wir die beiden Variablen.

```{r}
plot(Sales ~ Population, data = Carseats, 
     main = "Sales vs Population", pch=21, bg = "lightblue")
```

### Model erstellen und Auswertung

```{r}
lm.fit2 <- lm(Sales ~ Population, data = Carseats)
summary(lm.fit2)
```

Die Einfluss der Population ist nicht signifikant. Der folgende Plot bestätigt, dass die Steigung 0 beträgt. Wir werden diese Variable deshalb nicht weiter untersuchen.

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ Population, data = Carseats, 
     main = "Sales vs Population", pch=21, bg = "lightblue")
abline(lm.fit2, col = "red")
```

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit2) 
```

```{r}
hist(lm.fit2$residuals, 
     main = "Verteilung der Residuen", col = "lightblue") 
```
### Interpretation
viel blabla blaaaaa


## Sales vs. Advertising

### Visualisierung von Sales und Population

Bevor das Modell erstellt wird, visualisieren wir die beiden Variablen.

```{r}
plot(Sales ~ Advertising, data = Carseats, 
     main = "Sales vs Advertising", pch=21, bg = "lightblue")
```

### Modell erstellen und Auswertung

```{r}
lm.fit3 <- lm(Sales ~ Advertising, data = Carseats)
summary(lm.fit3)
```

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)

```{r}
plot(Sales ~ Advertising, data = Carseats, 
     main = "Sales vs Advertising", pch=21, bg = "lightblue")
abline(lm.fit3, col = "red")
```

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit3) 
```

```{r}
hist(lm.fit3$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```
### Interpretation
viel blabla blaaaaa


## Interpretation von Hypothese 1

Interperation: Sales ist nicht abhaengig von Advertising. Erkennbar im Tukeyanscombe Plot, Histogramm ist irreführend.

Idee: Kaum Zusammenänge, deshalb überprüfen wir bei Hypothese 2, ob eine Berechnung von Daten aussagekräftiger ist.


# Hypothese 2: Einfach lineare Regression mit berechneten Daten

Hat die Differenz von Comprice und Price einen Einfluss auf den Verkauf? (Lineare Regression)

## Sales vs. Preisdifferenz zu Konkurrenz

### Berechnung der Spalte DiffPrice

Hinzufügen einer Spalte mit der Differenz zwischen unserem Preis und der der Konkurrenz.
```{r}
Carseats <- Carseats %>% mutate(DiffPrice = Price - CompPrice) 
str(Carseats)
```


### Visualisierung von Sales und DiffPrice

```{r}
plot(Sales ~ DiffPrice, data = Carseats, 
     main = "Sales vs DiffPrice", pch=21, bg = "lightblue")
```
### Model erstellen und Auswertung
```{r}
lm.fit4 <- lm(Sales ~ DiffPrice, data = Carseats)
summary(lm.fit4)
```

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ DiffPrice, data = Carseats, 
     main = "Sales vs DiffPrice", pch=21, bg = "lightblue")
abline(lm.fit4, col = "red")
```

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit4) 
```


```{r}
hist(lm.fit4$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```
## Interpretation der Hypothese 2
auch blabla blaaaa


# Hypothese 3: Multiple lineare Regression

## Sales vs. mehrere Features

Ist Sales durch mehrere Features abhaengig?

### Modelle erstellen und Auswertungen
In diesem Kapitel erstellen wir mehrere Modelle und wählen eines aufgrund einer ersten Auswertung für die weitere Untersuchung aus.

#### Sales vs. Income und Population

```{r}
mlr.fit1 <- lm(Sales ~ Income + Population, data = Carseats)
summary(mlr.fit1)
```

#### Sales vs. Income und Population und deren Multiplikation (Interaktion)

```{r}
mlr.fit2 <- lm(Sales ~ Income * Population, data = Carseats)
summary(mlr.fit2)
```

#### Sales vs. alle Variabeln

```{r}
mlr.fit3 <- lm(Sales ~ ., data = Carseats)
summary(mlr.fit3)
```
Kommentar: Population hat definitv keinen Einfluss auf Sales, wir überprüfen deshalb andere Variablen.
DiffPrice haben wir als Differenz von CompPrice und Price berechnet. Das lineare Modell erkennt, dass DiffPrice in diesem Fall keine zusätzliche Information zum Modell beiträgt. Deshalb weist es ein NA aus. 

#### Sales vs. aussagekräftigste Variabeln und DiffPrice

```{r}
mlr.fit4 <- lm(Sales ~ Income + Advertising + ShelveLoc + Age + DiffPrice, data = Carseats)
summary(mlr.fit4)
```
Durch die Auswahl der aussagekräftigsten Variabeln konnten wir ein Modell erstellen, dass Sales zuverlässig erklärt. Die verwendeten Variabeln haben wir aus dem Modell, das alle Features verwendet, entnommen. Diese haben dort den tiefsten P-Wert.

### Residuenanalyse

```{r}
par(mfrow = c(2,2))
plot(mlr.fit4)
```

```{r}
hist(mlr.fit4$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```
## Interpreation von Hypothese 3
blaaaaasss


# Hypothese 4: Logistische Regression
Werden in urbanen Regionen mehr Kindersitze gekauft?

## Sales vs. Urban

### Klassifizierung erstellen

```{r}
Carseats <- Carseats %>% 
  mutate(Urban_binaer = recode(Urban, "No" = 0, "Yes" = 1))
```

### Visualisierung von Sales und Urban
```{r}
# Plot Predicted data and original data points
ggplot(Carseats, aes(x=Sales, y=Urban)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "Urban",
       title = "Sales vs Urban",
       subtitle = "Carseats Datensatz")
```
Eine Logistische Regression von Urban (Yes, No) ist mittels Sales nicht sinnvoll, da die Streupunkte
uebereinander liegen.

### Model erstellen und Auswertung
```{r}
logreg.fit1 <- glm(Urban_binaer ~ Sales, data = Carseats , family = binomial)
summary(logreg.fit1)
```
Wie Aufgrund der Visualisierung gesehen, ergibt ein logistishes Regressionsmodell einen
grossen P>|z| Wert. Somit gibt es keinen logistischen Zusammenhang zwischen Sales und Urban. 

## Sales vs. ShelveLoc

### Visualisierung von Sales und ShelveLoc
Zuerst für alles ShelveLoc, danach haben die Location "Medium" entfernt.
```{r}
Carseats %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) +
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "Urban",
       title = "Sales vs Urban",
       subtitle = "Carseats Datensatz")

# Remove Medium in ShelveLoc
Carseats_ex_medium <- Carseats %>% filter(ShelveLoc != "Medium")

Carseats_ex_medium %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "Urban",
       title = "Sales vs Urban",
       subtitle = "Carseats Datensatz")
```

### Klassifizierung erstellen
```{r}
Carseats_ex_medium <- Carseats_ex_medium %>% 
  mutate(ShelveLoc_binaer = recode(ShelveLoc, "Bad" = 0, "Good" = 1))
head(Carseats_ex_medium)
```

### Model erstellen und Auswertung
```{r}
logreg.fit2 <- glm(ShelveLoc_binaer ~ Sales, data = Carseats_ex_medium , family = binomial)
summary(logreg.fit2)
```

### Visualisierung der Daten mit dem Regressions Modell
```{r}
ggplot(Carseats_ex_medium, aes(x=Sales, y=ShelveLoc_binaer)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "Urban",
       title = "Sales vs Urban",
       subtitle = "Carseats Datensatz") +
  stat_smooth(method="glm", formula = y ~ x, color="red", 
              se = FALSE, method.args = list(family=binomial)) + 
  geom_vline(xintercept = 7.9633)

# Berechnung der Decision Boundary
#-logreg.fit2$coefficients[1] / logreg.fit2$coefficients[2]

```
Alle Sales die groesser als 7.9633 sind, werden bei der Logistischen Regression als Gut definiert. 

### Evaluierung vom logistischen Model mittels confusions Matrix
Wir erstellen ein neues logistisches Model und verwenden dafuer einen Trainingsdatensatz


#### Confusion Matrix erstellen
```{r}
# zuerst erstellen wir eine preidiction, mit den Testdaten

# Predicte ShelveLoc_binaer Werte 
logreg.fit2.probs <- predict(logreg.fit2, type = "response")

# Erstellt einen Vektor mit 181  Null Werten 
logreg.fit2.pred <- rep(0, 181)

# Uebreschreibt alle Werte die groesser sind als 0.5 als 1
logreg.fit2.pred[logreg.fit2.probs > .5] = 1

# Erstellen eine Kreuztabelle (Confusions Matrix) 

table(logreg.fit2.pred, Carseats_ex_medium$ShelveLoc_binaer)

```
Spalten = Carseats_ex_medium$Shelvelo_binear => Tatsaechlichen Werte
Zeilen => Vorhergesagte Werte


#### Confusion Matrix Performenz Kennzahlen
##### Accuracy
```{r}
# Accuracy Berechnen
acc <- (82 + 66) / (82 + 19 + 14 + 66)
acc

```

##### Precision
Precision = True Positiv / (True Positiv + False Positiv)
Preicision ist, Anteil richtig positiv an allen Positiv
```{r}
# Precision Berechnen 
precision <- 66 / (66 + 14)
precision
```
##### Reall
Recall = True Positiv / (True Positiv + False Negativ)
Recall ist, Anteil richtig positiv an allen richtigen Vorhersagen
```{r}
# Recall berechnen
recall <- 66 / (66 + 19)
recall
```