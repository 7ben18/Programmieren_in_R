---
title: "Programmieren in R: Abgabe 1"
author: "Gabriel Torres Gamez, Si Ben Tran, Patrick Schürmann"
output:
  html_document:
    toc: true
    toc_float: true
---

# Lineare und logistische Regressionen in R

Für die erste Abgabe im Modul PRR beschäftigen wir uns mit (multiplen) linearen und
logistischen Regressionen. Diese Funktionen wenden wir auf den Datensatz "Carseats" an. Datensatz von ISRL2.
Wir haben uns für diese Methoden entschieden, weil sie ein zentraler Bestandteil von Data Science sind. Sie wird verwendet, um Zusammenhängen in Datensätze zu verstehen und um Prognosen erstellen zu können. Diese Regressionen sind im R Grundpakte enthalten. Es muss also keine separate Library installiert und geladen werden.

Carseats ist ein **generischer** Datensatz, der im Buch "An Introduction to Statistical Learning" verwendet wird. Dieses Buch erarbeiten wir in diesem Studiengang im Modul "Statistisches Lernen". Dort wird er zwar hauptsächlich angewendet um Decision Trees zu erstellen. Dank 11 Variablen und 400 Observationen eignet er sich aber auch gut für Regressionen. Jede Zeile im Datensatz entspricht einem Verkaufslokal, gejoint mit Durchschnittsstatitistiken für das Einzugsgebiet des Lokals. Für eine genau Erklärung der Variablen verweisen wir auf die Homepage https://rdrr.io/cran/ISLR2/man/Carseats.html. Für die unten stehenden Hypothesen werden wir mit folgenden Variablen arbeiten:

- Sales: Verkaufte Anzahl Kindersitze pro Standort in 1'000 Stück
- CompPrice: Verkaufte Anzahl Kindersitze von Konkurrenten in 1'000 Stück
- Income: Das Lohnniveau der Einzugsgebiete der Standorte in 1'000 Dollar
- Advertising: Lokale Marketingausgaben in 1'000 Dollar
- Population: Bevölkerungsgrösse der Standorte in 1'000 Personen
- CompPrice: Die Preise für Kindersitze der lokalen Konkurrenten
- Price: Preis pro Kindersitz an den Standorten
- ShelveLoc: Ein Faktor der die Qualität des Produktes einteilt
- Age: Durchschnittsalter der lokalen Bevölkerung
- Education: Bildungslevel an jedem Standort
- Urban: Klassifikation, ob sich der Standort in einer städtischen oder ländlichen Region befindet
- US: Ein Indikator der angibt, ob sich das Geschäft in US befindet oder nicht

# Lineare Regression
Bei der Linearen Regression versucht man eine Zielvariable in Abhängigkeit durch **eine** andere Variable vorherzusagen. Das daraus entstehende Modell wird als eine Gerade dargestellt mit der

Formel: y = f(x) = ax + b = $\beta_{0}$ + $\beta_{1}$x = mx + n 

Das Wissen wird immersiv durch die Module llr, mlr und stl abgedeckt.

# Lineare Multiple Regression
Bei der Linearen Multiple Regression versucht man eine Zielvariable in Abhängigkeit durch **mehrere** andere Variable vorherzusagen. 

Formel ausgeschrieben: y = f(X) = $\beta_{0}$ + $\beta_{1}$  $x_{1}$ + $\beta_{2}$  $x_{2}$ + ... + $\beta_{n}$  $x_{n}$

Formel vektorisiert: y = f(X) = X $\cdot$ $\vec{\beta_{n}}$ + $\beta_{0}$

Das Wissen wird immersiv durch die Module llr, mlr und stl abgedeckt.

# Logistische Regression
Bei der Logistischen Regression versucht man eine Binäre Zielvariable in Abhängigkeit durch **eine** andere Variable vorherzusagen. 

Sigmoid Formel: $\sigma(z) = \frac{1}{1 + e^{-z}}$

Formel fuer logistische Regression: f(x) = $\sigma(ax + b)$ = $\frac{1}{1 + e^{-(ax + b)}}$

Das Wissen wird immersiv durch die Module llr, mlr und stl abgedeckt.

# Auflistung der Hypothesen

Zur Analyse des Datensatzes haben wir uns folgende Fragen gestellt:\n

1. Sales ist abhängig von Income/Advertising/Population. (Lineare Regression)\n

2. Die Differenz von Comprice und Price hat einen Einfluss auf den Verkauf. (Lineare Regression)\n

3. Sales lässt sich durch die Verwendung von mehreren Features zuverlässiger vorhersagen. (Multiple Lineare Regression)\n

4. Bewohner von urbanen Regionen kaufen weniger Kindersitze als Bürger ländlicher Regionen.\n

# Vorbereitung
In diesem Kapitel installieren wir die notwendigen Libraries und verschaffen uns einen Überlick über den Carseats Datensatz.

## Installationen
```{r}
# Libraries installieren und laden
library(ISLR2)
library(tidyverse)
```
In der Library ISLR2 befindet sich der Datensatz. 
Tidyverse wird für die Darstellung des Datensatzes und Visualisierungen benötigt.

## Überblick Datensatz Carseats
In einem ersten Schritt ist es wichtig den Datensatz zu verstehen, bevor Visualisierungen und Modelle erstellt werden.

### Daten einlesen
```{r}
# Carseats Daten einlesen
data(Carseats)
```

### Kopfzeile Carseats ausgeben 
```{r}
# Die ersten 6 Oberservationen ausgeben
head(Carseats)
```
### Selektieren von Spalten nach bestimmten Reihenfolgen
```{r}
Carseats <- Carseats %>% select(Sales, Price, CompPrice, Advertising, ShelveLoc, Urban, US,
                    Population, Income, Age, Education,)
```

### Fusszeile des Datensatzes ausgeben
```{r}
# Die letzten 6 Observationen ausgeben
tail(Carseats)
```
Durch die Ausgabe von Kopfzeile und Fusszeile kommen wir mit dem Datensatz in Berührung und können ein Bild der Attribute sowie Observationen machen.

### Informationen zum Datensatz herausfinden
```{r}
# Informationen zum DataFrame ausgeben
str(Carseats)
```
Durch str wird und die grösse des Dataframes angezeigt, sowie die Anzahl an Features (Spalten). Weiter erkennen wir von jeder Spalte den Datenyp und auch einige Werte die in der Spalte vorkommen. 

### Reihenfolge von ShelveLoc definieren
```{r}
Carseats$ShelveLoc <- factor(Carseats$ShelveLoc, ordered = TRUE, levels = c("Good", "Medium", "Bad"))
```

### Summarische Zusammenfassung von Carseats
```{r}
# Statistische Kennzahlen 
summary(Carseats)
```
wir geben von jeder numerischen Spalte die summarischen Statistiken aus. 
Darin enthalten sind: Minimum, 1. Quantil, Median, Mittelwert, 3. Quantil, Maximum.
Im summary() werden auch die Anzahl fehlende Werte angezeigt. 
In unserem Fall besitzt der Datensatz keine fehlende Werte.
Die Aufbereitung der Daten entfällt deshalb.

### Visualisierung der Daten

#### Verteilung der Sales
```{r}
ggplot(data = Carseats, aes(x = Sales)) + 
  geom_density(fill = "lightgreen", color = "black", alpha = 0.3) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```

Hier in diesem Dichte-Diagramm erkennen wir die Verteilung der numerischen Variabel Sales.
Die y Achse ist die Wahrscheinlichkeit für das Auftreten einer numerischen Sales Variabel. 
Aus diesem Grund wird die Dichtefunktion auch oft Wahrscheinlichkeitsdichtefunktion genannt. 
Sie ist eine gegelättete Version des Histogramm und wird oft nach dem gleichen Konzept verwendet. 

#### Verteilung der Sales anhand der Variable Urban
```{r}
ggplot(data = Carseats, aes(x = Sales, fill = Urban)) + 
  geom_density(alpha = 0.3) + 
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales",
       subtitle = "Carseats Datensatz")
```
Analog zur ersten Visualisierung betrachten wir hier die Dichteverteilung von Sales mit der Auftrennung
von Urban (ja/nein), sprich können wir mit diesem Plot unterschiede zwischen Urbanen Regionen (städtische oder ländliche)
Sales betrachten. In unserem Fall gibt es keinen grosse Unterschied, man erkennt ganz leicht, das 
Sales in nicht ländlichen Regionen diese im Sales Bereich grösser ist als bei Städtischen Regionen. 

#### Multiplots Verteilungen von Sales
```{r}
ggplot(Carseats, aes(x = Sales, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Sales",
       y = "Anzahl",
       title = "Verteilung der Sales unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```
In diesem Multiplot unterteilen wir die Dichteverteilung nach weiteren Kategorien, 
wie Urban und ShelveLoc. Auch unterteilen wir farblich, ob sich das Geschäft in den US 
befindet oder nicht, damit wir ein Gefühl für die Sales Verteilung mit verschiedenen 
Kategorischen Werten kriegen. Aufällig ist, dass die Verteilungen bei ShelveLoc Bad 
deutlich weiter Links sind (Mittelwert tiefer als bei ShelveLoc Medium und Good). 
Auch erkennen wir bei ShelveLoc Good, dass die Kindersitze in den nicht US Ländern 
weniger verkauft werden als in den US selber. 

#### Multiplots Verteilungen von Price
```{r}
ggplot(Carseats, aes(x = Price, fill = US)) + 
  geom_density(alpha = 0.3) +
  facet_grid(Urban  ~ ShelveLoc, labeller = label_both) +
  labs(x = "Price",
       y = "Anzahl",
       title = "Verteilung von Price unterteilt nach ShelveLoc und Urban",
       subtitle = "Carseats Datensatz")

```
Analog zum Sales multiplot erstellen wir ein Multiplot von Price, um eine Übersicht von Price 
mit unterschiedlichen Kategorien zu kriegen. Wir erkennen, dass bei ShelveLoc Good und Urbanen 
Regionen die Preise in den US deutlich höher sind als in nicht US Ländern. Auch erkennen wir
bei ShelveLoc Bad und nicht Urbanen Regionen, das die Preise für einen Kindersitz bei nicht US
Ländern teurer sind.

# Hypothese 1: Sales ist abhängig von Income/Advertising/Population der Region.
Thema: Einfache lineare Regression

## Sales vs. Income
### Visualisierung von Sales und Income

```{r}
plot(Sales ~ Income, data = Carseats, 
     main = "Sales vs Income", pch=21, bg = "lightblue")
```

Bevor wir mit der Linearen Regression beginnen, erstellen wir zuerst ein Streudiagramm von beiden Variabeln, um die Zusammenhänge zu visualiseren. Hier erkennen wir im Plot, dass die Variabeln Einkommen und Sales nicht besonders stark korrelieren, da die Punkte überall im Plot verstreut sind. 

### Modell erstellen
```{r}
lm.fit1 <- lm(Sales ~ Income, data = Carseats)
```

Mit der Funktion "lm()" wird das Lineare Modell erstellt und trainiert.

### Auswertung vom Modell
```{r}
summary(lm.fit1)
```

R hat nun die linear Regression erstellt. Die wichtigsten Werte lassen sich mit der Methode summary() anzeigen. Der Intercept beträgt 6.44 und die Steigung 0.015. Pro 1'000 Dollar höheres Einkommen steigt der Verkauf um 15 Stück. Dies erscheint im Verhältnis zum Intercept wenig, es müsste darüber diskutiert werden, ob dieser Erkenntnis für einen möglichen Kunden aussagekräftig ist.Anhand vom P Wert (0.00231) lässt sich aber feststellen, dass die Steigung siginfikant ist. Das Einkommen der Region hat also einen Einfluss auf die Verkaufszahlen. Vor einer endgültigen Aussage müssen aber weitere Untersuchungen, wie die Überprüfung der Residuen, durchgeführt werden. Diese Untersuchung führen wir später durch.

### Übersicht der Modell Methoden
```{r}
names(lm.fit1)
```
Das Modell enthält weitere Berechnungen und Kennzahlen. Names() zeigt eine Übersicht dieser Methoden.

### Koeffzienten vom Modell ausgeben
```{r}
lm.fit1$coefficients
```

Die oben ersichtlichen Koeffizienten lassen sich direkt via $coefficients abrufen.

### Konfidenzintervall für Koeffizienten angeben
```{r}
confint(lm.fit1)
```

Wertvoll ist die Berechnung der Konfidenzintervalle, die im Summary nicht angezeigt wird. Diese gibt an, in welchem Bereich sich die Koeffizienten mit einer Wahrscheinlichkeit von 95% befinden.

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ Income, data = Carseats, 
     main = "Sales vs Income", pch=21, bg = "lightblue")

abline(lm.fit1, col = "red")
```

Das Modell lässt sich direkt mit der Plotfunktion darstellen. Mit abline() lassen sich Linien zeichnen, in diesem Fall die lineare Regression. Im Plot erkennen wir, dass Income und Sales eine Punktwolke bilden. Sales streut auf der ganzen X Achse von maximalen bis minimalen beobaachteten Wert (0 - 15). Entsprechend korrelieren beide Variabeln wenig miteinander. Es gibt keinen klaren Zusammenhang. 

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)  
```

Nun, da wir unser Modell haben, können wir feststellen, ob es ein gutes Modell ist, indem wir bei den Residuen folgende 3 Bedingungen überprüfen:

- Der Fehler sollte nicht abhängig sein
- Der Erwartungswert sollte bei 0 liegen.
- Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. Der Median der Residuen entspricht -0.18, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

Ueberpreufen vom Erwartungswert = 0 und Normalverteilt:
```{r}
hist(lm.fit1$residuals, 
     main = "Verteilung der Residuen", col = "lightblue") 
```
### Interpretation

Hypothese: Sales ist abhängig von Income?

Wir erkennen aufgrund des Streudiagramm, dass eine Korrelation zwischen einkommen und Sales kaum vorhanden sind. Das Lineare Modell hat einen R^2 von 0.02, dies bestaetigt die Aussage. 

Aus diesem Grund ist Sales nicht abhaengig von Income.

## Sales vs. Population
### Visualisierung von Sales und Population
```{r}
plot(Sales ~ Population, data = Carseats, 
     main = "Sales vs Population", pch=21, bg = "lightblue")
```

Bevor das Modell erstellt wird, visualisieren wir die beiden Variabeln Population und Sales in eine Streudiagramm.

### Model erstellen und Auswertung
```{r}
lm.fit2 <- lm(Sales ~ Population, data = Carseats)
summary(lm.fit2)
```

Die Einfluss der Population ist nicht signifikant. Der folgende Plot bestätigt, 
dass die Steigung 0 beträgt. 

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ Population, data = Carseats, 
     main = "Sales vs Population", pch=21, bg = "lightblue")
abline(lm.fit2, col = "red")
```

Hier in diesem Plot stellen wir die Population mit Sales sowie dem Modell zusammen. Wir erkennen, dass die Punkte sich deutlich von der roten Linie (Model) streuen. Das Problem besteht analog zu Sales und Income.

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit2) 
```

Auch in diesem Model ueberpruefen wir die folgenden Kriterien des Models.

- Der Fehler sollte nicht abhängig sein
- Der Erwartungswert sollte bei 0 liegen.
- Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. Der Median der Residuen entspricht -0.0597, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

```{r}
hist(lm.fit2$residuals, 
     main = "Verteilung der Residuen", col = "lightblue") 
```
### Interpretation

Hypothese: Sales ist abhängig von Population?

Leider auch hier, aehnlich wie zu Sales vs. Income erkennen wir keinen Trend. Der R^2 Score betraegt 0.002547, also auch kein Gutes Modell. 

Aus diesem Grund, stimmt auch diese Hypothese nicht.

## Sales vs. Advertising
### Visualisierung von Sales und Population
```{r}
plot(Sales ~ Advertising, data = Carseats, 
     main = "Sales vs Advertising", pch=21, bg = "lightblue")
```

Bevor das Modell erstellt wird, visualisieren wir die beiden Variabeln Advertising und Sales.

### Modell erstellen und Auswertung
```{r}
lm.fit3 <- lm(Sales ~ Advertising, data = Carseats)
summary(lm.fit3)
```

### Visualisierung der Daten mit dem Regressions Modell
```{r}
plot(Sales ~ Advertising, data = Carseats, 
     main = "Sales vs Advertising", pch=21, bg = "lightblue")
abline(lm.fit3, col = "red")
```

Leider erkennen wir schon hier, dass es keinen Trend von Sales vs. Advertising vorhanden ist. Vollstaendigkeitshalber fuehren wir die Analysen trotzdem durch.

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit3) 
```

Auch in diesem Model ueberpruefen wir die folgenden Kriterien des Models.

- Der Fehler sollte nicht abhängig sein
- Der Erwartungswert sollte bei 0 liegen.
- Die Residuen sollten einer Normalverteilung folgen

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. Der Median der Residuen entspricht -0.1037, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

```{r}
hist(lm.fit3$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```

### Interpretation

Hypothese: Sales ist abhängig von Advertising?

Leider auch bei Advertising und Sales ist kein linearer Zusammenhang erslichtlich. In der 3. Hypothese beschaeftigen wir uns mit der multiplen Linearen Regression, diese wird bestimmt besser sein als bei der Linearen Regression. 

## Interpretation von Hypothese 1

Keiner der Linearen Modelle hat einen Linearen Zusammenhang. Mit einem einfachen Linearen Modell ist es nicht moeglich Sales vorhherzusagen. 

# Hypothese 2: Die Differenz von Comprice und Price hat einen Einfluss auf den Verkauf.

Thema: Einfach lineare Regression mit berechneten Daten

## Sales vs. Preisdifferenz zu Konkurrenz
### Berechnung der Spalte DiffPrice

Hinzufügen einer Spalte mit der Differenz zwischen unserem Preis und der der Konkurrenz.

```{r}
Carseats <- Carseats %>% mutate(DiffPrice = Price - CompPrice) 
str(Carseats)
```

### Visualisierung von Sales und DiffPrice
```{r}
plot(Sales ~ DiffPrice, data = Carseats, 
     main = "Sales vs DiffPrice", pch=21, bg = "lightblue")
```

In dieser Visualisierung von DiffPrice und Sales erkennen wir, dass die Streupunkte einem gewissen Trend folgen. 

### Model erstellen und Auswertung
```{r}
lm.fit4 <- lm(Sales ~ DiffPrice, data = Carseats)
summary(lm.fit4)
```

Die Auswertung zeigt, dass dieses Modell einen R^2 von 0.3575 hat. Dies ist nicht besonders gut, dennoch besser als die ersten drei Modelle.

### Visualisierung der Daten mit dem Regressions Modell (Visualsierung ausbessern)
```{r}
plot(Sales ~ DiffPrice, data = Carseats, 
     main = "Sales vs DiffPrice", pch=21, bg = "lightblue")
abline(lm.fit4, col = "red")
```

Hier in diesem Diagramm erkennen wir, dass das Modell eine negative Steigung hat. Dies erkennen wir einerseits an der Steigung im Summary, aber auch bei der Visualisierung. 

### Residuenanalyse
```{r}
par(mfrow = c(2, 2))
plot(lm.fit4) 
```

Auch in diesem Modell ueberpruefen wir die folgenden Kriterien des Modells.

- Der Fehler sollten unabhängig sein.
- Der Erwartungswert sollte bei 0 liegen.
- Die Residuen sollten einer Normalverteilung folgen.

Das mittlere Kriterium lässt sich aus der Summary Funktion ablesen. Der Median der Residuen entspricht -0.2678, dieses Kriterium ist erfüllt.

Auch dafür bietet R einfache Möglichkeiten an. Mit der folgenden beiden Funktionen lassen sich vier unterschiedliche Überprüfungen der Residuen anzeigen.

Im ersten Plot oben links (Tukeyanscombe Plot) wird ersichtich, dass die Residuen/Fehler unabhängig von einander sind und nicht keinem Muster folgen. Auch dieses Kriterium ist somit erfüllt.

Die Verteilung der Residuen lässt sich einfach im Histogramm überprüfen, ob deren Mittelwert sich um 0 Verteilt.

```{r}
hist(lm.fit4$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```

## Interpretation der Hypothese 2

Hypothese: Die Differenz von Comprice und Price hat einen Einfluss auf den Verkauf.

Die zweite Hypothese hat einen deutlich besseren R^2 Score verglichen zu den ersten drei Modellen bei der Hypothese 1. Die Differenz von Comprice und Price hat keinen grossen Einfluss auf Sales, weil das Bestimmtheitsmass 0.3575 betraegt. 

Es ist interessant zu erkennen, dass der Preisunterschied zwischen der Konkurrenz und dem Produkt einen ganz leichten negativen Trend zeigt. Nichts destotrotz, sind wir mit diesem Modell nicht zufrieden und begeben uns direkt in die Hypothese 3. 

# Hypothese 3: Sales lässt sich durch die Verwendung von mehreren Features zuverlässiger vorhersagen.

Thema: Multiple lineare Regression

## Sales vs. mehrere Features

Nachdem wir festgestellt haben, dass sich Sales nicht durch einzelne Variabeln erklären lasst, untersuchen wir die Möglichkeit, ob Sales sich durch mehrere Features erklaeren laesst. Wir werden sie zuerst gegenüber zwei Variabel untersuchen. Danach schauen wir uns den ganzen Datensatz an und behalten danach die aussagekräftigsten Variabeln.

### Modelle erstellen und Auswertungen

In diesem Kapitel erstellen wir mehrere Modelle und wählen eines aufgrund einer ersten Auswertung für die weitere Untersuchung aus.

#### Sales vs. Income und Population
```{r}
mlr.fit1 <- lm(Sales ~ Income + Population, data = Carseats)
summary(mlr.fit1)
```

Zuerst untersuchen wir Sales gegenüber Income und der Population. In der Auswertung wird ersichtlich, dass Population keinen Zusammenhang aufweist. R^2 spricht ebenfalls gegen einen Zusammenhang in dieser Auswertung.

Die F-Statistik sagt aus, dass mit hoher Wahrscheinlichkeit (p-Wert < 0.6%) mindestens eine Variabel einen Einfluss hat. In unserem Fall ist das Income, was wir schon aus vorhergehenden Modellen wissen.

#### Sales vs. Income und Population und deren Multiplikation (Interaktion)
```{r}
mlr.fit2 <- lm(Sales ~ Income * Population, data = Carseats)
summary(mlr.fit2)
```

Hier haben wir untersucht, ob die Variabeln Income und Population nicht nur additiv einen Einfluss haben, sondern ob sie auch gemeinsam Wirken (Interaktion/Synergie). Das ist in diesem Fall aber nicht gegeben. 

#### Sales vs. alle Variabeln
```{r}
mlr.fit3 <- lm(Sales ~ ., data = Carseats)
summary(mlr.fit3)
```

Mit dem Code "Sales ~ ." können wir ein Regressionsmodell Sales gegenüber allen Variabeln erstellen.

Dank der Auswertung sehen wir weiterhin, dass sieben Variabeln in diesem Modell zum Sales beitragen. Population, Education und Location (Urban und US) haben keinen Einfluss. Wir werden deshalb die aussagekräftigsten vertieft überprüfen.

Die Fehlermeldung bei DiffPrice kommt daher, dass haben wir sie als Differenz von CompPrice und Price berechnet haben. Das lineare Modell erkennt, dass DiffPrice in diesem Fall keine zusätzliche Information zum Modell beiträgt. Deshalb weist es ein NA aus. 

#### Sales vs. aussagekräftigste Variabeln und DiffPrice
```{r}
mlr.fit4 <- lm(Sales ~ Income + Advertising + ShelveLoc + Age + DiffPrice, data = Carseats)
summary(mlr.fit4)
```

Durch die Auswahl der aussagekräftigsten Variabeln konnten wir ein Modell erstellen, dass Sales zuverlässig erklärt. Die verwendeten Variabeln haben wir aus dem Modell, das alle Features verwendet, entnommen. Diese haben dort den tiefsten P-Wert. In diesem Modell haben wir nur (auch in der F-Statistik) sehr tiefe p-Werte und ein ziemlich hohes R^2 von 0.87. Wir überprüfen deshalb nun die Residuen.

### Residuenanalyse
```{r}
par(mfrow = c(2,2))
plot(mlr.fit4)
```

Im Q-Q Plot wird bestätigt, dass es sich um ein lineares Modell handelt. Der erste Plot und das folgende Histogram bestätigen ebenfalls, dass die Residuen normalverteilt und einen Mittelwert von 0 aufweisen.

```{r}
hist(mlr.fit4$residuals, 
     main = "Verteilung der Residuen", col = "lightblue")
```

## Interpreation von Hypothese 3

Hypothese: Sales lässt sich durch die Verwendung von mehreren Features zuverlässiger vorhersagen.

Unser letztes Modell hat gezeigt, dass mit einer sorgfältigne Auswahl der Variabeln Sales gut und zuverlässig erklärt werden kann. In der Praxis könnten noch mehr Kombinationen (Variabeln, Interaktionen, exponentielle Zusammenhänge etc...) untersucht werden. In dieser Abgabe lassen wir das weg, weil es sich lediglich um Wiederholungen der gezeigten Funktionen handelt.

# Hypothese 4: Bewohner von urbanen Regionen kaufen weniger Kindersitze als Bürger ländlicher Regionen.

Thema: Logistische Regression

## Sales vs. Urban

### Klassifizierung erstellen

Um logistische Regressionen durchzuführen, benötigen wir binäre Variabeln. Im ersten Schritt ersetzen wir Yes/No von Urban durch 1/0.

```{r}
Carseats <- Carseats %>% 
  mutate(Urban_binaer = recode(Urban, "No" = 0, "Yes" = 1))
```

### Visualisierung von Sales und Urban
```{r}
ggplot(Carseats, aes(x=Sales, y=Urban)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "Urban",
       title = "Sales vs Urban",
       subtitle = "Carseats Datensatz")
```

Da wir in den Plots zur logistischen Regression die Sigmoid Funktion einzeichnen möchten, verwenden wir diese Darstellung bei sämtliche logistischen Regression. 

Dank der visuellen Analyse von Sales und Urban wird ersichtlich, dass Urban:Yes in beide Richtungen stärker streut als Urban:No. Entsprechend wird das Modell kaum verlässliche Werte ergeben.

### Model erstellen und Auswertung
```{r}
logreg.fit1 <- glm(Urban_binaer ~ Sales, data = Carseats , family = binomial)
summary(logreg.fit1)
```

Wie wir aufgrund der Visualisierung erkannt haben, ergibt ein logistishes Regressionsmodell einen grossen P>|z| Wert. Somit gibt es keinen logistischen Zusammenhang zwischen Sales und Urban.

Gegenüber der lineare Regression haben wir den Parameter "family = binomial" erfasst.

## Sales vs. ShelveLoc

Um ein Beispiel für eine logistische Regression zeigen zu können, untersuchen wir auch Sales und ShelveLoc

### Visualisierung von Sales und ShelveLoc

Zuerst für alles ShelveLoc, danach haben die Location "Medium" entfernt.

```{r}
Carseats %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) +
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "ShelveLoc",
       title = "Sales vs ShelveLoc",
       subtitle = "Carseats Datensatz")

Carseats_ex_medium <- Carseats %>% filter(ShelveLoc != "Medium")

Carseats_ex_medium %>% 
  ggplot(aes(x = Sales, y = ShelveLoc)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "ShelveLoc",
       title = "Sales vs ShelveLoc",
       subtitle = "Carseats Datensatz")
```

Im ersten Plot wurde drei Variabeln dargestellt. Für ein gutes Vorzeigebeispiel werden wir nur Good und Bad
berücksichtigen.

### Klassifizierung erstellen
```{r}
Carseats_ex_medium <- Carseats_ex_medium %>% 
  mutate(ShelveLoc_binaer = recode(ShelveLoc, "Bad" = 0, "Good" = 1))
head(Carseats_ex_medium)
```

Wir haben "Medium" weggelassen und wieder eine binäre Klassifizierung erstellt.

### Model erstellen und Auswertung
```{r}
logreg.fit2 <- glm(ShelveLoc_binaer ~ Sales, data = Carseats_ex_medium , family = binomial)
summary(logreg.fit2)
```

Der sehr tiefe P-Wert weist auf eine hohe Zuverlässigkeit vom Modell hin.

Im folgenden Plot wird ersichtlich, dass das Modell bei allen Läden mit Sales > 7.96 den ShelveLoc als Good klassifiziert. Die Berechnung des Wertes haben wir im folgenden Code Chunk auskommentiert.

### Visualisierung der Daten mit dem Regressions Modell
```{r}
ggplot(Carseats_ex_medium, aes(x=Sales, y=ShelveLoc_binaer)) + 
  geom_point(color = "blue") +
  labs(x = "Sales",
       y = "ShelveLoc",
       title = "Sales vs ShelveLoc",
       subtitle = "Carseats Datensatz") +
  stat_smooth(method="glm", formula = y ~ x, color="red", 
              se = FALSE, method.args = list(family=binomial)) + 
  geom_vline(xintercept = 7.9633)

# Berechnung der Decision Boundary
#-logreg.fit2$coefficients[1] / logreg.fit2$coefficients[2]
```

### Evaluierung vom logistischen Model mittels Confusions Matrix
Um die Genauigkeit des Modelles zu berechnen, erstellen wir eine Confusion Matrix.

#### Confusion Matrix erstellen
```{r}
# zuerst erstellen wir eine Prediction, mit den Testdaten

# Predicte ShelveLoc_binaer Werte 
logreg.fit2.probs <- predict(logreg.fit2, type = "response")

# Erstellt einen Vektor mit 181  Null Werten 
logreg.fit2.pred <- rep(0, 181)

# Uebreschreibt alle Werte die groesser sind als 0.5 als 1
logreg.fit2.pred[logreg.fit2.probs > .5] = 1

# Erstellen eine Kreuztabelle (Confusions Matrix) 
table("Actual Class" = Carseats_ex_medium$ShelveLoc_binaer, "Predicted Class" = logreg.fit2.pred)

```

Die Spalten entsprechen den tatsächlichen Werten und in den Zeilen befinden sich die vorhergesagten Werte.
Lesebeispiel: Das Modell hat 19 Werte als 0/Bad klassifiziert, die aber tatsächlich ein 1/Good gewesen sind.

#### Confusion Matrix Performenz Kennzahlen
##### Accuracy

Die Accuracy entspricht dem Anteil korrekter Prognosen an allen Prognosen.
```{r}
# Accuracy Berechnen
acc <- (82 + 66) / (82 + 19 + 14 + 66)
acc

```

##### Precision
Precision = True Positiv / (True Positiv + False Positiv)
Preicision ist, Anteil richtig positiv an allen Positiv

Precision entspricht der Anteil korrekter positiven/1 Prognosen an allen positiven Prognosen.

```{r}
# Precision Berechnen 
precision <- 66 / (66 + 14)
precision
```

##### Recall / Sensitivity
Recall = True Positiv / (True Positiv + False Negativ)
Recall ist, Anteil richtig positiv an allen richtigen Vorhersagen

Recall / Sensitivity entspricht der Anteil korrekter positiven/1 Prognosen an allen tatsächlich positiven Werten.

```{r}
# Recall berechnen
recall <- 66 / (66 + 19)
recall
```

# Zusammenfassung

Mit unseren verwendeten Modellen konnten wir aufzeigen, dass Sales nicht von einzelnen Variabeln, aber von mehreren zusammenhängt. Dank der multiplen linearen Regression konnten wir zuverlässigen Einflussfaktoren aufzeigen.

Dank der logistischen Regression konnten wir darstellen, dass Urban keinen Einfluss hat. Mit dem Weglassen von Medium konnten wir für ShelveLoc ein zuverlässiges Modell erstellen.

